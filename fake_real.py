# -*- coding: utf-8 -*-
"""fake/real.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x9p4jICncczzX6iXCh2HtYBUF_Yio8qC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import csv
import tensorflow as tf
import tensorflow_hub as hub

df_fake=pd.read_csv("/content/Fake.csv")
df_real=pd.read_csv("/content/True.csv")

df_fake.columns

df_fake.head()

df_fake['subject'].value_counts()

import seaborn as sns
sns.countplot(x='subject',data=df_fake)

df_fake['text'].tolist()

df_real.sample(5)

publication=[]
for index, row in enumerate(df_real.text.values):
  try:
    record = row.split('-',maxsplit=1)
    record[1]

    assert(len(record[0])<120)
  except:
      publication.append(index)

len(publication)

df_real=df_real.drop(8970,axis=0)

publisher=[]
tmp_text=[]
for index, row in enumerate(df_real.text.values):
  if index in publication:
    tmp_text.append(row)
    publisher.append('unknown')
  else:
    record=row.split('-',maxsplit=1)
    publisher.append(record[0].strip())
    tmp_text.append(record[1].strip())

df_real['publisher']=publisher
df_real['text']=tmp_text

df_real.head()

df_real.shape

empty_fake=[index for index,text in enumerate(df_fake.text.tolist()) if str(text).strip()==""]

df_real['text']=df_real['title']+' '+df_real['text']
df_fake['text']=df_fake['title']+' '+df_fake['text']

df_real["text"].apply(lambda x:str(x).lower())
df_fake["text"].apply(lambda x:str(x).lower())

df_real["output"]=1
df_fake["output"]=0

df_real.columns

df_real=df_real[['text','output']]
df_fake=df_fake[['text','output']]
df_real.shape, df_real.shape

data=df_real.append(df_fake,ignore_index=True)
data.shape

!pip install text-preprocessing

import text_preprocessing as ps

data['text'].apply(lambda x:ps.remove_special_character(x))
data['text'].apply(lambda x:ps.to_lower(x))

y=data["output"]
y

X= [d.split() for d in data['text'].tolist()]

import gensim
DIM=100

w2v_model=gensim.models.Word2Vec(sentences=X,vector_size=DIM,window=10,min_count=1)

from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)
X = tokenizer.texts_to_sequences(X)
X

tokenizer.word_index

import matplotlib.pyplot as plt
plt.hist([len(x) for x in X],bins=700)
plt.show()

!pip install Keras-Preprocessing

arr=np.array([len(x) for x in X])
len(arr[arr>1000])

from keras_preprocessing.sequence import pad_sequences
maxlen=1000
X =pad_sequences(X, maxlen=maxlen)
len(X[101])

vocab_size=len(tokenizer.word_index)+1
vocab=tokenizer.word_index

def get_weight_matrix(model):
  weight_matrix=np.zeros((vocab_size,DIM))
  for word, i in vocab.items():
    weight_matrix[i]=model.wv[word]
    return weight_matrix

embedding_vectors = get_weight_matrix(w2v_model)

embedding_vectors.shape

from keras.models import Sequential
from keras.layers import Flatten, Dense,LSTM
from keras.layers import Embedding
model=Sequential()
model.add(Embedding(vocab_size,output_dim=DIM,weights=[embedding_vectors],input_length=maxlen,trainable=False))
model.add(LSTM(units=256))
tf.keras.layers.Dense(256, activation='relu'),
tf.keras.layers.Dropout(0.4)
model.add(Dense(1,activation='sigmoid'))

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])

model.summary()

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,y)

model.fit(X_train,Y_train,validation_split=0.3,epochs=5)

